# Common Environment Variables for Hadoop + Spark Cluster.  
# 容器共用环境变量

# ***********Spark Section - Spark部分***********
# Reference: bitnami/spark

SPARK_RPC_AUTHENTICATION_ENABLED=no
SPARK_RPC_ENCRYPTION_ENABLED=no
SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
SPARK_SSL_ENABLED=no

# ***********Hadoop Mode Section - Hadoop启动模式配置部分***********

# general - Generally distributed cluster 普通分布式
# ha - High Availability cluster 高可用集群模式
HADOOP_LAUNCH_MODE=general 

# ***********Hadoop Regular Section - Hadoop普通分布式配置部分***********

# Hadoop Master Node Host 主节点主机名
# HDFS Namenode will be launched on this node
# HDFS Namenode将会在这个节点上启动
HADOOP_MASTER=shmain 

# Hadoop Worker Node Hosts (list) 从节点主机名（列表）
# HDFS DataNode and Yarn NodeManager will be launched on each of these nodes
# HDFS DataNode和Yarn NodeManager 将会在这些节点上启动
HADOOP_WORKERS="shworker1 shworker2"

# Whether to Launch HDFS DataNode on Master node
# 是否在主节点上启动HDFS DataNode
DN_ON_MASTER=true

# HDFS Replication 副本数
HDFS_REPLICATION=2 

# Yarn Resourcemanager lies here
# Yarn Resourcemanager 所在结点
YARN_RM_NODE="shworker2"

# Whether to Launch Yarn NodeManager on $YARN_RM_NODE
# 是否在YARN_RM_NODE上启动Yarn NodeManager
NM_WITH_RM=false

# Start HDFS on cluster startup
# 容器集群启动时顺带启动HDFS集群
HDFS_LAUNCH_ON_STARTUP=true

# Start Yarn on cluster startup
# 容器集群启动时顺带启动Yarn集群
YARN_LAUNCH_ON_STARTUP=true

# ***********Others 其他配置部分***********

