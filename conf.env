# Common Environment Variables for Hadoop + Spark Cluster.  
# 容器共用环境变量


# ***********Primary Configuration - 首要配置***********

# All hosts in the cluster
# 集群中所有主机的主机名
SH_HOSTS="shmain shworker1 shworker2"


# ***********Spark Section - Spark部分***********
# Reference: bitnami/spark

SPARK_RPC_AUTHENTICATION_ENABLED=no
SPARK_RPC_ENCRYPTION_ENABLED=no
SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
SPARK_SSL_ENABLED=no

# ***********Hadoop Common Section - Hadoop通用配置部分***********

# general - Generally distributed cluster 普通分布式
# ha - High Availability cluster 高可用集群模式
HADOOP_LAUNCH_MODE=ha 

# HDFS Replication HDFS副本数
HADOOP_HDFS_REPLICATION=2 

# The amount of memory to request from the scheduler for each map task. (in MiB)
# 为每个Map任务分配的内存量 (以MiB为单位)
HADOOP_MAP_MEMORY_MB=1024

# The amount of memory to request from the scheduler for each reduce task. (in MiB)
# 为每个Reduce任务分配的内存量 (以MiB为单位)
HADOOP_REDUCE_MEMORY_MB=1024

# ***********Hadoop General Section - Hadoop普通分布式配置部分***********

# Hadoop HDFS Master Node Host 主节点主机名
# HDFS Namenode will be launched on this node
# HDFS Namenode将会在这个节点上启动
GN_NAMENODE_HOST=shmain 

# Hadoop Worker Node Hosts (list) 从节点主机名（列表）
# HDFS DataNode and Yarn NodeManager will be started on each of these nodes
# HDFS DataNode 和 Yarn NodeManager 将会在这些节点上启动
GN_HADOOP_WORKER_HOSTS="shworker1 shworker2"

# Whether to Launch HDFS DataNode on Master node
# 是否在主节点上启动HDFS DataNode
GN_DATANODE_ON_MASTER=true

# Secondary Namenode lies here. Remain empty to disable it.
# Secondary Namenode 所在结点，留空则不启动Secondary Namenode
GN_SECONDARY_DATANODE_HOST=shworker1

# Yarn Resourcemanager lies here
# Yarn Resourcemanager 所在结点
GN_RESOURCEMANAGER_HOST=shworker2

# Whether to Launch Yarn NodeManager on $GN_RESOURCEMANAGER_HOST
# 是否在GN_RESOURCEMANAGER_HOST上启动Yarn NodeManager
GN_NODEMANAGER_WITH_RESOURCEMANAGER=true

# Whether to setup HDFS on container startup
# 容器集群启动时顺带启动HDFS集群
GN_HDFS_SETUP_ON_STARTUP=true

# Whether to setup Yarn on container startup
# 容器集群启动时顺带启动Yarn集群
GN_YARN_SETUP_ON_STARTUP=false



# ***********Hadoop High Availability Section - Hadoop高可用分布式配置部分***********

# Nameservice of HDFS HA Cluster
# HDFS高可用集群的服务名（逻辑地址）
HA_HDFS_NAMESERVICE="ha-hdfscluster"

# Whether to initialize and start the HDFS cluster with the container.
# 是否随容器启动，初始化并启动HDFS高可用集群
# PS: If this item is set to 'false', the HDFS high availability cluster will not be deployed.
# PS: 如果这一项为'false'，HDFS高可用集群将不会被部署。
HA_HDFS_SETUP_ON_STARTUP="true"

# The list of hostnames for starting the NameNode, separated by spaces.
# 启动NameNode的节点主机名列表，空格分隔
# PS: The ZKFC process starts along with the NameNode.
# PS: ZKFC进程会随着NameNode进程启动。
HA_NAMENODE_HOSTS="shmain shworker1 shworker2"  

# The list of hostnames for starting the JournalNode, separated by spaces.
# 启动JournalNode的节点主机名列表，空格分隔
# Suggested: Use an odd number of hostnames, at least: 3
# 建议采用奇数个JournalNode节点，至少3个
HA_JOURNALNODE_HOSTS="shmain shworker1 shworker2"  

# The list of hostnames for starting the DataNode, separated by spaces.
# 启动DataNode的节点主机名列表，空格分隔
HA_DATANODE_HOSTS="shmain shworker1 shworker2"

# Whether to initialize and start the Yarn cluster with the container.
# 是否随容器启动，初始化并启动Yarn高可用集群
# PS: If this item is set to 'false', the Yarn high availability cluster will not be deployed.
# PS: 如果这一项为'false'，Yarn高可用集群将不会被部署。
HA_YARN_SETUP_ON_STARTUP="true"

# Yarn Cluster ID
# Yarn集群ID
HA_YARN_CLUSTER_ID="ha-rmcluster"

# The list of hostnames for starting the ResourceManager, separated by spaces.
# 启动ResourceManager的节点主机名列表，空格分隔
HA_RESOURCEMANAGER_HOSTS="shmain shworker2"  

# The list of hostnames for starting the NodeManager, separated by spaces.
# 启动NodeManager的节点主机名列表，空格分隔
HA_NODEMANAGER_HOSTS="shmain shworker1 shworker2"  